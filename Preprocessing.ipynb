{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUwk0htAKlL4",
        "outputId": "0ee961e6-725e-4876-ea0b-0fe83585199a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnLWi6AMMDiQ",
        "outputId": "d72f9ff1-2a2f-4e9b-fb4e-3335857f5e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nGKBvaaMTYs",
        "outputId": "c84f6d3a-10d7-49e0-ba82-38ef6639a513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: False\n",
            "GPU Name: No GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBw2WmsXMTTE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynrKmwB7MOLN"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "params = {\n",
        "    'tree_method': 'gpu_hist',   # Enables GPU\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'objective': 'binary:logistic'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU0ZG2rHLm8_"
      },
      "source": [
        "PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2_uxEErLV4",
        "outputId": "024e16a2-da04-4611-ac72-7b413f7baef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before casting:\n",
            "root\n",
            " |-- feature_1: double (nullable = true)\n",
            " |-- feature_2: double (nullable = true)\n",
            " |-- feature_3: string (nullable = true)\n",
            " |-- feature_4: double (nullable = true)\n",
            " |-- feature_5: string (nullable = true)\n",
            " |-- feature_6: integer (nullable = true)\n",
            " |-- feature_7: double (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n",
            "After casting:\n",
            "root\n",
            " |-- feature_1: double (nullable = true)\n",
            " |-- feature_2: double (nullable = true)\n",
            " |-- feature_3: double (nullable = true)\n",
            " |-- feature_4: double (nullable = true)\n",
            " |-- feature_5: double (nullable = true)\n",
            " |-- feature_6: double (nullable = true)\n",
            " |-- feature_7: double (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n",
            "Numeric columns: []\n",
            "Categorical columns: []\n",
            "Warning: No features were created - check your input data\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install and import PySpark (if in Colab)\n",
        "!pip install -q pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 2: Start Spark session\n",
        "spark = SparkSession.builder.appName(\"PDC_Preprocessing\").getOrCreate()\n",
        "\n",
        "# Step 3: Load the CSV file\n",
        "df = spark.read.csv(\"/content/pdc_dataset_with_target.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# OPTIONAL: Check schema before transformation\n",
        "print(\"Before casting:\")\n",
        "df.printSchema()\n",
        "\n",
        "# Step 4: Cast columns to appropriate types - don't convert string columns to double\n",
        "for col_name in df.columns:\n",
        "    if col_name != \"target\" and str(df.schema[col_name].dataType) != 'StringType':\n",
        "        df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
        "\n",
        "# Confirm schema\n",
        "print(\"After casting:\")\n",
        "df.printSchema()\n",
        "\n",
        "# Step 5: Identify numeric and categorical columns\n",
        "numeric_cols = [field.name for field in df.schema.fields if str(field.dataType) == 'DoubleType' and field.name != 'target']\n",
        "categorical_cols = [field.name for field in df.schema.fields if str(field.dataType) == 'StringType']\n",
        "\n",
        "print(\"Numeric columns:\", numeric_cols)\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "\n",
        "# Step 6: Build preprocessing stages\n",
        "pipeline_stages = []\n",
        "\n",
        "# --- Impute numeric columns with mean ---\n",
        "if numeric_cols:\n",
        "    imputer = Imputer(inputCols=numeric_cols, outputCols=[c + \"_imputed\" for c in numeric_cols])\n",
        "    pipeline_stages.append(imputer)\n",
        "    imputed_cols = [c + \"_imputed\" for c in numeric_cols]\n",
        "else:\n",
        "    imputed_cols = []\n",
        "\n",
        "# --- Encode categorical columns using StringIndexer ---\n",
        "if categorical_cols:\n",
        "    indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "    pipeline_stages += indexers\n",
        "    indexed_cat_cols = [col + \"_indexed\" for col in categorical_cols]\n",
        "else:\n",
        "    indexed_cat_cols = []\n",
        "\n",
        "# --- Assemble all features ---\n",
        "if imputed_cols or indexed_cat_cols:  # Only assemble if we have features\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=imputed_cols + indexed_cat_cols,\n",
        "        outputCol=\"features_unscaled\"\n",
        "    )\n",
        "    pipeline_stages.append(assembler)\n",
        "\n",
        "    # --- Standardize features ---\n",
        "    scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withMean=True, withStd=True)\n",
        "    pipeline_stages.append(scaler)\n",
        "\n",
        "    # Step 7: Build and apply pipeline\n",
        "    pipeline = Pipeline(stages=pipeline_stages)\n",
        "    model = pipeline.fit(df)\n",
        "    processed_df = model.transform(df)\n",
        "\n",
        "    # Step 8: Final dataset with features and target\n",
        "    final_df = processed_df.select(\"features\", \"target\")\n",
        "\n",
        "    # Show a few rows to verify\n",
        "    final_df.show(5, truncate=False)\n",
        "else:\n",
        "    print(\"Warning: No features were created - check your input data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcIq0pLvLmfh",
        "outputId": "a04b853f-4f1e-4196-bc65-e1c9058a2610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Preprocessing done.\n",
            "Feature shape: (41000, 7)\n",
            "Target shape: (41000,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/pdc_dataset_with_target.csv\")\n",
        "\n",
        "# Split types\n",
        "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop('target')\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# --- Step 1: Handle Missing Values in Parallel ---\n",
        "\n",
        "def impute_column(col):\n",
        "    if df[col].dtype in ['float64', 'int64']:\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "    else:\n",
        "        imputer = SimpleImputer(strategy='most_frequent')\n",
        "    df[col] = imputer.fit_transform(df[[col]])\n",
        "    return df[col]\n",
        "\n",
        "# Parallel imputation\n",
        "_ = Parallel(n_jobs=-1)(\n",
        "    delayed(impute_column)(col) for col in df.columns if df[col].isnull().sum() > 0\n",
        ")\n",
        "\n",
        "# --- Step 2: Encode Categorical Variables ---\n",
        "\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# --- Step 3: Normalize Numerical Features ---\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# --- Final Dataset ---\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Output shapes\n",
        "print(\"✅ Preprocessing done.\")\n",
        "print(\"Feature shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pytJo24-4onb",
        "outputId": "6862da9d-d692-477d-b9f2-4b53348962a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature_1    float64\n",
            "feature_2    float64\n",
            "feature_3     object\n",
            "feature_4    float64\n",
            "feature_5     object\n",
            "feature_6      int64\n",
            "feature_7    float64\n",
            "target         int64\n",
            "dtype: object\n",
            "feature_1    float32\n",
            "feature_2    float32\n",
            "feature_3     object\n",
            "feature_4    float32\n",
            "feature_5     object\n",
            "feature_6      int32\n",
            "feature_7    float32\n",
            "target         int32\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/pdc_dataset_with_target.csv')\n",
        "print(data.dtypes)\n",
        "\n",
        "data['feature_6'] = data['feature_6'].astype('int32')\n",
        "data['feature_1'] = data['feature_1'].astype('float32')\n",
        "data['feature_2'] = data['feature_2'].astype('float32')\n",
        "data['feature_4'] = data['feature_4'].astype('float32')\n",
        "data['feature_7'] = data['feature_7'].astype('float32')\n",
        "data['target'] = data['target'].astype('int32')\n",
        "\n",
        "print(data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCSOMv6N6QKR",
        "outputId": "53f94cb7-f173-4a34-9642-c26903f8031d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature_1    2054\n",
            "feature_2    2050\n",
            "feature_3       0\n",
            "feature_4    2054\n",
            "feature_5       0\n",
            "feature_6       0\n",
            "feature_7    2036\n",
            "target          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQGxxKsC7TCv",
        "outputId": "554b7476-dc41-48b5-e574-a7a54097ed44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature_1    0\n",
            "feature_2    0\n",
            "feature_3    0\n",
            "feature_4    0\n",
            "feature_5    0\n",
            "feature_6    0\n",
            "feature_7    0\n",
            "target       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Fixing all missing values without chained assignment\n",
        "\n",
        "data['feature_1'] = data['feature_1'].astype(float)\n",
        "data['feature_1'] = data['feature_1'].fillna(data['feature_1'].mean())\n",
        "\n",
        "data['feature_2'] = pd.to_numeric(data['feature_2'], errors='coerce')\n",
        "data['feature_2'] = data['feature_2'].fillna(data['feature_2'].mean())\n",
        "\n",
        "data['feature_4'] = pd.to_numeric(data['feature_4'], errors='coerce')\n",
        "data['feature_4'] = data['feature_4'].fillna(data['feature_4'].mean())\n",
        "\n",
        "data['feature_7'] = pd.to_numeric(data['feature_7'], errors='coerce')\n",
        "data['feature_7'] = data['feature_7'].fillna(data['feature_7'].mean())\n",
        "\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q1J_TAHykvM"
      },
      "outputs": [],
      "source": [
        "data.to_csv('Preprocessed.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
